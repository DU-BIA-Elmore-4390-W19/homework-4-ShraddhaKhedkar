---
title: 'Homework 4: Bags, Forests, Boosts, oh my'
author: "Shraddha Khedkar"
date: "2/28/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem 1

Problem 7 from Chapter 8 in the text. To be specific, please use a sequence of
`ntree` from 25 to 500 in steps of 25 and `mtry` from 3 to 9 for by 1. 

## Answer 1

```{r libs, message = F, warning = F, include = F}
library(tidyverse)
library(broom)
library(glmnet)
library(caret)
library(ISLR)
library(janitor)
library(stringr)
library(rpart)
library(rpart.plot)
library(partykit)
library(randomForest)
library(MASS)
library(gbm)
library(colorspace)
library(ggplot2)
theme_set(theme_bw())
```



```{r}
set.seed(1234)
df <- tbl_df(Boston)

for (k in 1:20){
  inTraining <- createDataPartition(df$medv, p = .75, list = F)
  training <- df[inTraining, ]
  testing <- df[-inTraining, ]
  mtry <- c(3:9)
  ntree <- seq(25, 500, len = 20)
  results <- tibble(trial = rep(NA, 140),
  mtry = rep(NA, 140),
  ntree = rep(NA, 140),
  mse = rep(NA, 140)) 
  for(i in 1:7){
    cat(sprintf('Trial: %s, mtry: %s --- %s\n', k, mtry[i], Sys.time()))
    for(j in 1:20){ 
      rf_train <- randomForest(medv ~ .,
                               data = training,
                               mtry = mtry[i],
                               ntree = ntree[j])
      mse <- mean((predict(rf_train, newdata = testing) - testing$medv)^2)
      results[(i-1)*20 + j, ] <- c(k, mtry[i], ntree[j], mse)
    }
  }
  if(exists("results_total")){
  results_total <- bind_rows(results_total, results)
  }
  else(
  results_total <- results
  )
}
```

```{r}
p <- ggplot(data = results,
            aes(x = ntree, y = mse, col = as.factor(mtry)))
p + geom_line() + 
  geom_point() +
  scale_color_brewer("mtry", palette = "Dark2")
```


## Problem 2

Problem 8 from Chapter 8 in the text. Set your seed with 9823 and split into 
train/test using 50\% of your data in each split. In addition to 
parts (a) - (e), do the following:

1. Fit a gradient-boosted tree to the training data and report the estimated 
test MSE. 
2. Fit a multiple regression model to the training data and report the 
estimated test MSE
3. Summarize your results. 

### Answer 2 (NEW)

### 2.a 

```{r}
library(tree)
library(ISLR)
attach(Carseats)
set.seed(9823)
df<- tbl_df(Carseats)
inTraining <- createDataPartition(df$Sales, p=.50, list= F)
training <- df[inTraining, ]
testing <- df[-inTraining, ]
```


```{r}
tree_carseats <- rpart::rpart(Sales ~ ., 
                              data = training,
                              control = rpart.control(minsplit = 20))
summary(tree_carseats)
prp(tree_carseats)
```

```{r}
plot(as.party(tree_carseats))
```

```{r}
pred_carseats = predict(tree_carseats, testing)
mean((testing$Sales - pred_carseats)^2)
```

The test MSE is 4.48


### 2.c

```{r}
fit_control <- trainControl(method = "repeatedcv",
                            number = 10, 
                            repeats = 10)
cv_tree_carseats <- train(Sales ~ ., 
                          data = training,
                          method = "rpart", 
                          trControl = fit_control)
plot(cv_tree_carseats)
```
```{r}
plot(as.party(cv_tree_carseats$finalModel))
```

```{r}
pred_carseats_1 = predict(cv_tree_carseats, testing)
mean((testing$Sales - pred_carseats_1)^2)
```
The test MSE is increased to 6.17 due to pruning

### 2.d

```{r}
bag_carseats <- randomForest(Sales ~ ., data = training, mtry = 10)
bag_carseats
```

```{r}
test_preds <- predict(bag_carseats, newdata = testing)
carseats_test_df <- testing %>%
  mutate(y_hat_bags = test_preds,
         sq_err_bags = (y_hat_bags - Sales)^2)
mean(carseats_test_df$sq_err_bags)
```
The test error rate on the bagging approach is 3.06. This reduction looks good. 

```{r}
importance(bag_carseats)
```

ShelveLoc, Price and CompPrice are the most important predictors of Sales.


### 2.e

```{r}
rf_carseats <- randomForest(Sales ~ ., 
                            data = training,
                            mtry = 10)
rf_carseats
```

```{r}
pred_carseats_3 = predict(rf_carseats, testing)
mean((testing$Sales - pred_carseats_3)^2)
```
The random forest MSE is even lower at 2.87.

```{r}
importance(rf_carseats)
```

The most important variables in the random forest model are ShelveLoc, Price, and Comp Price again liked the baggged appraoch.


### Answer 3

```{r}
grid <- expand.grid(interaction.depth = c(1, 3), 
                    n.trees = seq(0, 2000, by = 100),
                    shrinkage = c(.01, 0.001),
                    n.minobsinnode = 10)
trainControl <- trainControl(method = "cv", number = 5)
gbm_carseats <- train(Sales ~ ., 
                      data = training, 
                      distribution = "gaussian", 
                      method = "gbm",
                      trControl = trainControl, 
                      tuneGrid = grid,
                      verbose = FALSE)
gbm_carseats
```


```{r}
plot(gbm_carseats)
```



```{r}
pred_carseats_4 = predict(gbm_carseats, testing)
mean((testing$Sales - pred_carseats_4)^2)
```

The MSE is 1.801 and is better



### 3.2

```{r}
lm_carseats <- lm(Sales ~.,
                  data = training)
#Backwards setpwise regression
library(MASS)
step_carseats <- stepAIC(lm_carseats, direction='backward')
step_carseats$anova
pred_carseats_5 = predict(step_carseats, testing)
mean((testing$Sales - pred_carseats_5)^2)
```

The MSE is 1.01. This is the lowest


### 3.3

The backwards stepwise linear regression model is the best model of the methods with a testing mean square error of 1.01.

Model Mean Square Error Summary
b- Regression Tree MSE: 4.48
c- CV Pruned Regression Tree MSE: 6.17
d- Bagged Random Forest MSE: 3.06
e- Random Forest: 2.87
f- Gradient Boosted Model: 1.834
g- Backwards stepwise linear regression: 1.01.



